# Data-Warehouse
Data warehousing using  Redshift for Spakify data

# Data Warehousing with AWS Redshift

## Overview
A startup called spakify wants to analyze the data on songs and users obtained from the new music streaming app. The data collected is in json format. In this project we extracted the data from an S3 bucket and loaded it into two tables created in redshift (staging_events and staging_songs). The data loaded into  redshift was used to populate the dimension and fact table. 

## Datasets
The dataset used for this project are song and log datasets.
### 1. Song dataset
This is a subset of real data from the million song dataset. Each files is on json format and contains metadata about a song and the artist of the song.
### 2. Log dataset
This consist of log files in json format generated by the even simulator based on songs in the data set.

## Database Schema
A star scheme was used to model this database in AWS Redshift. It consist of a fact table and four dimention tables.
### 1. Fact Table
It consist of records in log data associated with songsplay i.e records with page 'Nextsong'.
### 2. Dimension Table
These tables are connected to the fact table 'songplays'. They include 'users','songs','artist' and 'time'.

## ETL pipeline
I extracted data form the song and log dataset and inserted them into the fact and dimension tables.

## Project File
1. **create_redshift_cluster.ipynb**- This file was used to create the redshift cluster using 'Infrastructure as code' (Python SDK)
2. **dwh.cfg**- Contains configurations information of IAM, Redshift cluster and S3 storage
3. **create_table.py**- It executes scripts to create the database connection, drop tables and creates table
4. **etl.py**- It executes scripts to load the tables with data and complete the ETL process.
5. **sql_query.py**- This script creates and inserts data into tables.
6. **READme**- Docuentation of the project.

### How to run the project
1. Execute the create_table.py script to drop table and previous database and create a new database with tables.
2. Execute the etl.py to extract the json file format in song and log dataset and inserts the data into the dimension and facts table.


